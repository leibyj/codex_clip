#training
epochs: 10
learning_rate: 0.001
weight_decay: 0.01
max_grad_norm: 1.0
text_lr: 1e-5
codex_lr: 5e-5
optimizer: "adam"
log_interval: 10
